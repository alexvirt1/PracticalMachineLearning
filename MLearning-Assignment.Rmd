---
title: "Practical Machine Learning"
author: "alexvirt"
date: "January 30, 2016"
output: html_document
---
#Executive Summary
Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in HAR with wearable accelerometers), especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises.

The data is available here:
http://groupware.les.inf.puc-rio.br/har

Training data:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

Test data:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv


```{r}
library(caret)
setwd('C:/Temp/R/MLearning')
```

Load training and sample data:

```{r, echo=TRUE}
training <- read.csv("pml-training.csv",na.strings = c("NA","#DIV/0!",""))
testing <- read.csv("pml-testing.csv",na.strings = c("NA","#DIV/0!",""))
```

#Clean Data and remove invalid predictors
Some predictors has NA values and zero values. Those should be excluded.
```{r, echo=TRUE}
# Remove columns with Near Zero Values
subTrain <- training[, names(training)[!(nzv(training, saveMetrics = T)[, 4])]]

# Remove columns with NA or is empty
subTrain <- subTrain[, names(subTrain)[sapply(subTrain, function (x) ! (any(is.na(x) | x == "")))]]

# Remove V1 and cvtd_timestamp, they are not good predictors either
subTrain <- subTrain[,-1]
subTrain <- subTrain[, c(1:3, 5:58)]
```

#Separate the data for Cross Validation

```{r, echo=TRUE}
# Divide the training data into a training set and a validation set
inTrain <- createDataPartition(subTrain$classe, p = 0.6, list = FALSE)
subTraining <- subTrain[inTrain,]
subValidation <- subTrain[-inTrain,]
```

#Create prediction model. 
Since it is very CPU intensive, it is better to calculate it once and the use the saved copy.
```{r, echo=TRUE}
if (!file.exists("savedFit.RData")) {
  fit <- train(subTraining$classe ~ ., method = "rf", data = subTraining)
} else {
  load(file = "savedFit.RData", verbose = TRUE)
}
```

#Measure the accuracy and the sample error of the prediction model
Create prediction basing on the training subset and measure it's accuracy.
```{r, echo=TRUE}
library(randomForest)
predTrain <- predict(fit, subTraining)
confusionMatrix(predTrain, subTraining$classe)
```
We can see that accuracy is very high - 99.94%


Using the validation subset and create a prediction. Then measure it's accuracy. From the training subset, the accuracy is very high, at above 99%. The sample error is 0.0008.
```{r, echo=TRUE}
predValidation <- predict(fit, subValidation)
confusionMatrix(predValidation, subValidation$classe)
```
From the validation subset the accuracy is 99.95% (confidence interval: 99.87% - 99.99%), it is very high. It means that we can accept this predicition model as correct.

From the model, the following are the list of important predictors in the model.
```{r, echo=TRUE}
varImp(fit)
fit$finalModel
```
The reported OOB Estimated Error is at 0.12%. 


#Apply the prediction model
Predict classification data for 20 test cases:

```{r, echo=TRUE}
predict(fit, testing)
```

#Conclusion
The prediction model was built with very high accuracy and low sample error. It allowed to predict values of 20 test cases.